# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - default.yaml

# common - for all tasks (task_name, tags, output_dir, device)
policy:
  alpha:
    _target_: builtins.tuple
    _args_:
      - - "neg_act_num" # target_entropy - float -6.0 or str "neg_act_num"
        - _target_: torch.zeros
          size: 
            _target_: builtins.tuple
            _args_:
              - - 1
          requires_grad: true
          device: ${device}
        - _target_: torch.optim.Adam
          _partial_: true
          lr: 3e-4 # 3e-4 in tianshou
actor:
  unbounded: true
  conditioned_sigma: true
actor_optim:
  lr: 1e-3 # 1e-3 in tianshou, 3e-4 in dongqi code. (For both actor and critic)

runner:
  _target_: src.runner.SACRunner


start_timesteps: 10000

algorithm_name: sac